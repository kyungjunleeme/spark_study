{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2a5d41-5825-4a4b-83a7-5937781e79ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark \n",
    "\n",
    "# # class pyspark.SparkContext (\n",
    "# #    master = None,\n",
    "# #    appName = None, \n",
    "# #    sparkHome = None, \n",
    "# #    pyFiles = None, \n",
    "# #    environment = None, \n",
    "# #    batchSize = 0, \n",
    "# #    serializer = PickleSerializer(), \n",
    "# #    conf = None, \n",
    "# #    gateway = None, \n",
    "# #    jsc = None, \n",
    "# #    profiler_cls = <class 'pyspark.profiler.BasicProfiler'>\n",
    "# # )\n",
    "\n",
    "# # Master − It is the URL of the cluster it connects to.\n",
    "# # appName − Name of your job.\n",
    "# # sparkHome − Spark installation directory.\n",
    "# # pyFiles − The .zip or .py files to send to the cluster and add to the PYTHONPATH.\n",
    "# # Environment − Worker nodes environment variables.\n",
    "# # batchSize − The number of Python objects represented as a single Java object. Set 1 to disable batching, 0 to automatically choose the batch size based on object sizes, or -1 to use an unlimited batch size.\n",
    "# # Serializer − RDD serializer.\n",
    "# # Conf − An object of L{SparkConf} to set all the Spark properties.\n",
    "# # Gateway − Use an existing gateway and JVM, otherwise initializing a new JVM.\n",
    "# # JSC − The JavaSparkContext instance.\n",
    "# # profiler_cls − A class of custom Profiler used to do profiling (the default is pyspark.profiler.BasicProfiler).\n",
    "\n",
    "# # SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext. \n",
    "# # By default, PySpark has SparkContext available as ‘sc’, so creating a new SparkContext won't work.\n",
    "\n",
    "# # 경준 추가\n",
    "# # Remote client cannot create a SparkContext. Create SparkSession instead.\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# # RDD : immutable distributed collection of objects\n",
    "# rdd = sc.parallelize(range(1000))\n",
    "# rdd.takeSample(False, 5)\n",
    "\n",
    "from databricks.connect import DatabricksSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate();\n",
    "\n",
    "from operator import add\n",
    "rdd = spark.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba683410-1c76-4dbc-a90a-0665272ca580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lecture1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
